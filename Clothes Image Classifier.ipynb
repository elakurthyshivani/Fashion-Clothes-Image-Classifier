{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T14:39:08.863030Z",
     "iopub.status.busy": "2022-03-14T14:39:08.862550Z",
     "iopub.status.idle": "2022-03-14T14:39:09.913932Z",
     "shell.execute_reply": "2022-03-14T14:39:09.913081Z",
     "shell.execute_reply.started": "2022-03-14T14:39:08.862944Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the training data\n",
    "<br />The file <code>train.csv</code> consists of image ID (from <code>/images</code> folder) and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T14:51:16.452518Z",
     "iopub.status.busy": "2022-03-14T14:51:16.451866Z",
     "iopub.status.idle": "2022-03-14T14:51:16.477982Z",
     "shell.execute_reply": "2022-03-14T14:51:16.477189Z",
     "shell.execute_reply.started": "2022-03-14T14:51:16.452476Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T14:53:56.941712Z",
     "iopub.status.busy": "2022-03-14T14:53:56.941023Z",
     "iopub.status.idle": "2022-03-14T14:54:02.756176Z",
     "shell.execute_reply": "2022-03-14T14:54:02.755278Z",
     "shell.execute_reply.started": "2022-03-14T14:53:56.941665Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all the images whose image ID is present in the training set.\n",
    "<br />Each image is adjusted to 32 x 32 pixels and is converted into an array.\n",
    "<br /><code>X_train</code> consists of all the training images (array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T14:54:24.136597Z",
     "iopub.status.busy": "2022-03-14T14:54:24.136307Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "height, width=32, 32\n",
    "X_train=np.empty((train.shape[0], height, width, 3))\n",
    "for i in range(train.shape[0]):\n",
    "    img=load_img(r\"images/images/{}.jpg\".format(train.loc[i, 'img_id']), \\\n",
    "                 target_size=(height, width))\n",
    "    X_train[i]=img_to_array(img)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the training data\n",
    "<br /><b>Standardizing</b> all the pixels of the images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m, s=X_train.mean(), X_train.std()\n",
    "X_train=(X_train-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.max(), X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>One-Hot encoding</b> of labels.\n",
    "<br /><code>y_train</code> consists of labels of all the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder()\n",
    "y_train=ohe.fit_transform(np.array(train['label']).reshape(-1, 1)).toarray()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, AveragePooling2D, Dropout, \\\n",
    "        BatchNormalization, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model using Convolutional Networks\n",
    "<br />The model consists of 4 <b>convolution layers</b> with 64, 64, 72, 72 number of nodes in each layer respectively and 2 <b>fully connected layers</b> with 256 and 64 nodes respectively. Final output layer consists of 4 nodes with activation <code>softmax</code>. All other layers (convolution and fully-connected) use <code>relu</code> activation.\n",
    "<br /><b>Average Pooling, Dropout,</b> and <b>Batch Normalization</b> are also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(Conv2D(64, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(72, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(Conv2D(72, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model runs for 125 epochs. A <b>validation set</b> is taken out with 25% of the training images and is used in training the model. <code>Adam</code> optimizier is used along with a batch size of 128 examples is used in fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs=125\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "stats=model.fit(X_train, y_train, epochs=epochs, batch_size=128, validation_split=0.25)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the training results - Categorical loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df=pd.DataFrame(stats.history)\n",
    "stats_df['epoch']=list(range(1, epochs+1))\n",
    "plot.figure(figsize=(20, 8))\n",
    "sb.lineplot(y='loss', x='epoch', data=stats_df, color='deeppink', linewidth=2.5, label=\"Training loss\")\n",
    "sb.lineplot(y='val_loss', x='epoch', data=stats_df, color='darkturquoise', linewidth=2.5, label=\"Validation loss\")\n",
    "plot.grid()\n",
    "plot.legend()\n",
    "plot.title(\"Training and validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df=pd.DataFrame(stats.history)\n",
    "stats_df.accuracy=stats_df.accuracy*100\n",
    "stats_df.val_accuracy=stats_df.val_accuracy*100\n",
    "stats_df['epoch']=list(range(1, epochs+1))\n",
    "plot.figure(figsize=(20, 8))\n",
    "sb.lineplot(y='accuracy', x='epoch', data=stats_df, color='deeppink', linewidth=2.5, label=\"Training accuracy\")\n",
    "sb.lineplot(y='val_accuracy', x='epoch', data=stats_df, color='darkturquoise', linewidth=2.5, \\\n",
    "            label=\"Validation accuracy\")\n",
    "plot.grid()\n",
    "plot.legend()\n",
    "plot.title(\"Training and validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the test data\n",
    "<br />The file <code>test.csv</code> consists of image ID (from <code>/images</code> folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r\"test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all the images whose image ID is present in the testing set.\n",
    "<br />Each image is adjusted to 32 x 32 pixels and is converted into an array.\n",
    "<br /><code>X_test</code> consists of all the testing images (array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test=np.empty((test.shape[0], height, width, 3))\n",
    "for i in range(test.shape[0]):\n",
    "    img=load_img(r\"images/images/{}.jpg\".format(test.loc[i, 'img_id']), \\\n",
    "                 target_size=(height, width))\n",
    "    X_test[i]=img_to_array(img)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the testing data\n",
    "<br /><b>Standardizing</b> all the pixels of the images in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m, s=X_test.mean(), X_test.std()\n",
    "X_test=(X_test-m)/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.max(), X_test.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the final model\n",
    "<br />Building the model once again with without taking out the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(Conv2D(64, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(72, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(Conv2D(72, kernel_size=3, padding=\"valid\", activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs=125\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "stats=model.fit(X_train, y_train, epochs=epochs, batch_size=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions from one-hot encoding format to the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label']=ohe.inverse_transform(predictions)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the predictions off to a <code>.csv</code> file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r\"Predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
